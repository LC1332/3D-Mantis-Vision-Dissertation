{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU5Pxyz/Uwy3xUOR+KUdJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/3D-Mantis-Vision-Dissertation/blob/main/notebook/novel_data/%E8%AF%BB%E5%8F%96%E5%B9%B6%E6%89%B9%E9%87%8F%E6%8A%BD%E5%8F%96%E5%8D%A1%E7%89%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfwDktxc-ad6",
        "outputId": "f309e8a6-278a-4fd0-db9b-d10dd92cf150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个pkl在\n",
        "\n",
        "https://drive.google.com/file/d/1LSVaMmKkfuHp1rm3DK0U2zosgZiv34dJ/view?usp=sharing"
      ],
      "metadata": {
        "id": "ZT9ZassQCEgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = \"/content/drive/MyDrive/CardBuild/exp0212/card2extract.pkl\" # 冷子昂改为你下载后的目录\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(input_name, 'rb') as f:\n",
        "    datas = pickle.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "zcotVj-h_VIg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_save_folder = \"/content/drive/MyDrive/CardBuild/exp0212/\" # 冷子昂改为你要保存的目录"
      ],
      "metadata": {
        "id": "VueYjOxYBVWp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "载入openai"
      ],
      "metadata": {
        "id": "qCVtbOgy_izj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtxdCWKp_XvX",
        "outputId": "7d7ef612-68b6-4589-c502-8660692d427e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/226.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import httpx\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"sk-u4ba9\" # 冷子昂改这个key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://p\" # 冷子昂 这个不设置就注释掉\n",
        "\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "model_name = \"gpt-4-0125-preview\" # 冷子昂改为 gpt-4-0125-preview"
      ],
      "metadata": {
        "id": "-253yQjl_lck"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q aiofiles tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1OHBVga_mk4",
        "outputId": "a4400f95-4e2d-4d60-b4e1-a0c85223479b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "# import openai\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "    aclient = AsyncOpenAI( api_key = os.getenv(\"OPENAI_API_KEY\" ) )\n",
        "else:\n",
        "    aclient = AsyncOpenAI( base_url = os.getenv(\"OPENAI_API_BASE\"), api_key = os.getenv(\"OPENAI_API_KEY\") )\n",
        "\n",
        "import asyncio\n",
        "import aiofiles\n",
        "import tiktoken\n",
        "import hashlib\n",
        "# from connector import AsyncPGConnector\n",
        "from tqdm.asyncio import tqdm as tqdm\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "en2zh_ratio = 2.3\n",
        "\n",
        "delay = 1\n",
        "concurrency_limit = 16\n",
        "\n",
        "max_file_size = 1024**3"
      ],
      "metadata": {
        "id": "Wif-bqqW_ytP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def dealing_messages(messages):\n",
        "    try:\n",
        "        # request_token = sum([len(enc.encode(msg['content'])) for msg in messages])\n",
        "        # response_token = int(len(enc.encode(text)) * en2zh_ratio) + 64\n",
        "\n",
        "        model = \"gpt-3.5-turbo-1106\"\n",
        "\n",
        "        resp = await aclient.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            response_format={ \"type\": \"json_object\" },\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            result = resp.choices[0].message.content\n",
        "            result = result.strip()\n",
        "            return result\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Invalid json: \", result)\n",
        "            return None\n",
        "        except:\n",
        "            raise Exception(f\"Invalid API response: {resp}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "_2EzquAa_6-G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def getTranslation(item):\n",
        "    if \"messages\" not in item:\n",
        "        return None\n",
        "    else:\n",
        "        for i in range(3):\n",
        "            result = await dealing_messages(item['messages'])\n",
        "            if result is not None:\n",
        "                item[\"response\"] = result\n",
        "                return item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "async def process(item, semaphore):\n",
        "    async with semaphore:\n",
        "        try:\n",
        "            output_folder = \"/content/output\"\n",
        "            if \"output_folder\" in item:\n",
        "                output_folder = item[\"output_folder\"]\n",
        "            if not os.path.exists(output_folder):\n",
        "                os.makedirs(output_folder)\n",
        "\n",
        "            file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                return\n",
        "\n",
        "            it = await getTranslation(item)\n",
        "            if it is None:\n",
        "                raise Exception(item['id'])\n",
        "\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(it, f, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing entry: {e}\")"
      ],
      "metadata": {
        "id": "YbBFEOt9ANQm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datas[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKfxA-lhAVx2",
        "outputId": "31953985-ce9d-4d59-eeb1-27dd4ae3f92c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'input'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main(datas):\n",
        "\n",
        "    output_folder = \"/content/output\"\n",
        "\n",
        "    process_data = []\n",
        "\n",
        "    for data in datas:\n",
        "        id = data['id']\n",
        "        process_data.append({\n",
        "            \"id\": id,\n",
        "            \"messages\": [{\"role\":\"user\",\"content\":data[\"input\"]}],\n",
        "            \"output_folder\": output_folder\n",
        "        })\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # # print(f\"Already processed {len(exist_list)} items...\")\n",
        "\n",
        "    # id = set()\n",
        "\n",
        "    for item in process_data:\n",
        "        file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            continue\n",
        "\n",
        "        tasks.append(asyncio.create_task(process(item, semaphore)))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)"
      ],
      "metadata": {
        "id": "oHUa1dUGAQ5t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_id = 0\n",
        "end_id = len(datas)\n",
        "\n",
        "current_tasks = datas[start_id:end_id]\n",
        "\n",
        "await main(current_tasks)\n",
        "\n",
        "temp_output_folder = \"/content/output\"\n",
        "\n",
        "for id in range(start_id, end_id):\n",
        "    id_str = datas[id][\"id\"]\n",
        "    file_path = os.path.join(temp_output_folder, f\"{id_str}.txt\")\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                data = json.load(f)\n",
        "                response = data[\"response\"]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if response is not None:\n",
        "            datas[id][\"response\"] = response\n",
        "\n",
        "\n",
        "final_save_name = jsonl_save_folder + str(start_id) + \"_to_\" + str(end_id) + \".txt\"\n",
        "\n",
        "with open(final_save_name, 'w', encoding='utf-8') as f:\n",
        "    for id in range(start_id, end_id):\n",
        "        json.dump(datas[id], f, ensure_ascii=False)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLAyYy_9AkKW",
        "outputId": "4981a6a3-6e55-4f0b-e71d-c5d19b1b728f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Rbl6UomBDZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}